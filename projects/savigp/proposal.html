<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="proposal.tex"> 
<meta name="date" content="2015-03-30 15:32:00"> 
<link rel="stylesheet" type="text/css" href="proposal.css"> 
</head><body 
>
   <h4 class="likesubsectionHead"><a 
 id="x1-1000"></a><span 
class="ptmrc7t-x-x-120">P<span 
class="small-caps">a</span><span 
class="small-caps">r</span><span 
class="small-caps">t</span> E &#8211; P<span 
class="small-caps">r</span><span 
class="small-caps">o</span><span 
class="small-caps">j</span><span 
class="small-caps">e</span><span 
class="small-caps">c</span><span 
class="small-caps">t</span> D<span 
class="small-caps">e</span><span 
class="small-caps">s</span><span 
class="small-caps">c</span><span 
class="small-caps">r</span><span 
class="small-caps">i</span><span 
class="small-caps">p</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">o</span><span 
class="small-caps">n</span></span></h4>
<!--l. 68--><p class="noindent" ><span 
class="ptmrc7t-x-x-120">E1. P<span 
class="small-caps">r</span><span 
class="small-caps">o</span><span 
class="small-caps">j</span><span 
class="small-caps">e</span><span 
class="small-caps">c</span><span 
class="small-caps">t</span> T<span 
class="small-caps">i</span><span 
class="small-caps">t</span><span 
class="small-caps">l</span><span 
class="small-caps">e</span>: </span>Scalable Inference for Probabilistic Joint Inversions, Regression, Classification and
Beyond.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-2000"></a><span 
class="ptmrc7t-x-x-120">E2. B<span 
class="small-caps">a</span><span 
class="small-caps">c</span><span 
class="small-caps">k</span><span 
class="small-caps">g</span><span 
class="small-caps">r</span><span 
class="small-caps">o</span><span 
class="small-caps">u</span><span 
class="small-caps">n</span><span 
class="small-caps">d</span></span></h5>
<!--l. 73--><p class="noindent" >Scientists are often concerned with inversion problems where they wish to reason about the latent input to a
system given some observations of its output and the system&#8217;s forward model. Similarly, they face predictive
tasks where, although reasoning about the latent inputs may not be of interest, probabilistic predictions are
required and it can be useful to incorporate problem-specific knowledge into the inference algorithm, for
example, through a likelihood model.
<!--l. 80--><p class="indent" >   It turns out that both problems, the inversion problem and the purely predictive problem, can be elegantly
modelled within a Bayesian framework by computing posterior distributions and posterior predictive
distributions, respectively. The challenge here is exactly this computation, as posterior inference is generally
intractable, involving the marginalisation over a large number of states (in the discrete case) or the
computation of high-dimensional integrals (in the continuous case). This problem is exacerbated when
dealing with nonlinear forward models (or likelihoods) as the interaction between the inputs and the outputs
can be quite complex.
<!--l. 87--><p class="indent" >   Two main approaches have been proposed in the literature to deal with the problem of approximate
Bayesian inference, the <span 
class="ptmri7t-x-x-120">stochastic </span>approach and the <span 
class="ptmri7t-x-x-120">deterministic </span>approach. The former, the stochastic
approach, is based on sampling algorithms such as Markov Chain Monte Carlo (MCMC, see e.g.&#x00A0;<span class="cite">[<a 
href="#Xneal1993probabilistic">4</a>]</span> for
an overview). The latter, the deterministic approach, is based on optimisation techniques and
include Variational Inference (VI, <span class="cite">[<a 
href="#Xjordan1998introduction">2</a>]</span>), the Laplace approximation and Expectation Propagation
(EP).
<!--l. 96--><p class="indent" >   MCMC algorithms provide a flexible framework for sampling from complex posterior distributions of
probabilistic models. However, their generality comes at the expense of very high computational cost as well
as cumbersome convergence analysis. Deterministic approaches, such as variational inference, can be
considerably faster than MCMC but they lack MCMC&#8217;s broader applicability as derivations of
the corresponding objective function and its gradients are required on a model-by-model basis.
<br 
class="newline" /><br 
class="newline" /><span 
class="ptmb7t-x-x-120">Project Aims. </span>Consequently, there is a tension between computational <span 
class="ptmri7t-x-x-120">efficiency </span>and <span 
class="ptmri7t-x-x-120">automation</span>. While
stochastic approaches can be fairly generic and automated, they are, in general, very slow. Conversely,
deterministic approaches can be fast but lack the generality of some stochastic methods. This trade-off is
illustrated in Figure <a 
href="#x1-20011">1<!--tex4ht:ref: fig:inference --></a>(a). In this project we aim at bridging this gap by developing efficient and highly
automated deterministic inference algorithms. In particular, we will consider Gaussian process (GP) priors in
order to address problems such as joint inversions, multi-output regression, multi-class classification and
modelling of count data in an automated manner. GPs are notorious for being flexible priors over functions
but also for their poor scalability to large datasets. Therefore, an additional component of this
project is <span 
class="ptmri7t-x-x-120">scalability </span>of our inference algorithms to deal with a large number of observations.
<br 
class="newline" /><br 
class="newline" /><span 
class="ptmb7t-x-x-120">Recent International Progress. </span>The general relation to the field of Bayesian inference has been described
above, and the proposed approach will attempt to go one-step further by automating and scaling up Bayesian
inference for models based on Gaussian process priors. However, not surprisingly, there is international
interest in working towards similar goals. In particular, Black box variational inference (BBVI, <span class="cite">[<a 
href="#Xranganath2014black">9</a>]</span>) has
recently been developed for general latent variable models. Due to this generality, it under-utilises the rich
                                                                                            
                                                                                            
amount of information available in GP models (see more details in Section E4). A clear disadvantage of
BBVI is that it does not provide an analytical or practical way of learning the covariance hyperparameters of
GPs. <hr class="figure"><div class="figure" 
>
                                                                                            
                                                                                            
<a 
 id="x1-20011"></a>
                                                                                            
                                                                                            
<div class="tabular"> <table id="TBL-1" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"><col 
id="TBL-1-2"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-1"  
class="td11">  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-2"  
class="td11">  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-1"  
class="td11">(a)</td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-2"  
class="td11">(b)</td></tr></table>
</div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1:  </span><span  
class="content">(a)  A  pictorial  representation  of  approximate  Bayesian  inference  methods.  Stochastic
approaches such as Markov Chain Monte Carlo (MCMC), Elliptical Sliced Sampling (ESS), Hybrid
Montecarlo  (HMC)  and  Gibbs  can  be  highly  automated  but  very  slow.  In  contrast,  deterministic
approaches  such  as  Variational  Inference  (VI),  Laplace,  and  Expectation  Propagation  (EP)  are
generally fast but require a lot of mathematical derivations on a model-specific basis. Our goal is
to develop highly-automated deterministic inference algorithms that can scale to very large datasets.
(b) A general framework for variational inference with Gaussian process (GP) priors based on the
optimisation of an evidence lower bound. </span></div><!--tex4ht:label?: x1-20011 -->
                                                                                            
                                                                                            
<!--l. 142--><p class="indent" >   </div><hr class="endfigure">
<!--l. 144--><p class="indent" >   Another approach closely related to ours is in <span class="cite">[<a 
href="#Xopper-arch-nc-2009">8</a>]</span>, which investigates variational inference for GP models
with one latent function and factorial likelihood. Our approach is more general in that it will allow
multiple latent functions, hence being applicable to settings such as multi-class classification,
multi-output regression, and joint inversions. Furthermore, we aim to scale up these algorithms so as to
deal with a large number of observations, allowing their application to large-scale data fusion
problems.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-3000"></a><span 
class="ptmrc7t-x-x-120">E3. S<span 
class="small-caps">i</span><span 
class="small-caps">g</span><span 
class="small-caps">n</span><span 
class="small-caps">i</span><span 
class="small-caps">fi</span><span 
class="small-caps">c</span><span 
class="small-caps">a</span><span 
class="small-caps">n</span><span 
class="small-caps">c</span><span 
class="small-caps">e</span> <span 
class="small-caps">a</span><span 
class="small-caps">n</span><span 
class="small-caps">d</span> I<span 
class="small-caps">n</span><span 
class="small-caps">n</span><span 
class="small-caps">o</span><span 
class="small-caps">v</span><span 
class="small-caps">a</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">o</span><span 
class="small-caps">n</span></span></h5>
<!--l. 171--><p class="noindent" >The research proposed in this project is highly significant across a variety of applications that demand
uncertainty quantification and propagation, see Section E5 for some examples. Similarly, efficient Bayesian
inference is an important problem of modern statistical data analysis. As we shall see in Section E5, the
anticipated outcomes of this project can be summarised as:
<ul class="itemize1">
<li class="itemize"><span 
class="ptmri7t-x-x-120">Automation   of   deterministic   posterior   inference</span>:   Developing   generic   deterministic   inference
algorithms for a variety of likelihood models (or forward models).
</li>
<li class="itemize"><span 
class="ptmri7t-x-x-120">Scalability of GP-based variational inference</span>: Scaling up these generic variational inference methods
for models with Gaussian process priors to deal with a large number of observations.</li></ul>
<!--l. 184--><p class="noindent" >If successful, the research in the project will advance the state-of-the-art deterministic posterior inference
algorithms in that it will eliminate the lengthy and tedious derivations by researchers when developing
inference algorithms on a model-by-model basis. It will also allow the applicability of these algorithms to
very large datasets, usually found in joint inversion problems, modern machine learning tasks and
&#8216;big-data&#8217; applications. For this, we will build upon the following insightful methodologies and
technologies:
<ul class="itemize1">
<li class="itemize"><span 
class="ptmri7t-x-x-120">Optimising rather than integrating: </span>The underpinning technology of our research for deterministic
posterior  approximation  is  variational  inference,  which  transforms  the  problem  of  computing
high-dimensional  integrals  into  an  optimisation  problem.  This  will  overcome  the  deficiencies  of
stochastic  approaches  (such  as  MCMC),  which  are  notorious  for  their  high  computational  cost,
especially in highly coupled models.
</li>
<li class="itemize"><span 
class="ptmri7t-x-x-120">Decomposition  of  the  objective  function:  </span>We  will  exploit  the  decomposition  of  the  variational
inference  objective  into  two  components,  a  Kullback-Leibler  (KL)  divergence  component  and  an
expected  log  likelihood  (ELL)  component.  In  particular,  by  having  Gaussian  process  priors  we
anticipate to be able to compute the KL component efficiently for some posterior distributions. For
the ELL component, we can build upon neat analytical results in the literature that, until now, have
not been sufficiently exploited for these types of models. See item below.
</li>
<li class="itemize"><span 
class="ptmri7t-x-x-120">Analytical results for expectations of factorised likelihoods: </span>We aim to generalise the results of <span class="cite">[<a 
href="#Xopper-arch-nc-2009">8</a>]</span> to
a larger class of models. In particular, these results state that for one-dimensional output problems,
it is possible to calculate the ELL term efficiently by computing expectations over 1-dimensional
Gaussian distributions.
                                                                                            
                                                                                            
</li>
<li class="itemize"><span 
class="ptmri7t-x-x-120">Summary statistics of GP models: </span>We will attempt to scale up our generic inference algorithms by
exploiting the concept of summary statistics, also known in the GP literature as <span 
class="ptmri7t-x-x-120">inducing points</span>, see
for example <span class="cite">[<a 
href="#Xnguyen-bonilla-uai-2014">6</a>]</span>.
</li>
<li class="itemize"><span 
class="ptmri7t-x-x-120">Stochastic  optimisation:  </span>We  will  combine  the  methodology  of  summary  statistics  along  with
the  decomposability  of  the  variational  objective  function  in  order  to  provide  a  scalable  learning
framework whose computationally complexity will not depend on the size of the dataset by using
stochastic optimisation. We will also investigate very recent advances in stochastic optimisation <span class="cite">[<a 
href="#XSchRouBac2013">10</a>]</span>
in order to improve the convergence of our learning algorithm.</li></ul>
<!--l. 221--><p class="noindent" >
   <h5 class="likesubsubsectionHead"><a 
 id="x1-4000"></a><span 
class="ptmrc7t-x-x-120">E4. A<span 
class="small-caps">p</span><span 
class="small-caps">p</span><span 
class="small-caps">r</span><span 
class="small-caps">o</span><span 
class="small-caps">a</span><span 
class="small-caps">c</span><span 
class="small-caps">h</span></span> <a 
 id="x1-4000doc"></a></h5>
<!--l. 222--><p class="noindent" >The aims of this project will be achieved through the development and extension of techniques
that are sound and have proved successful in uncertainty modelling. In particular, we will use
Bayesian reasoning to address the problem of quantification and propagation of uncertainty.
<br 
class="newline" /><br 
class="newline" /><span 
class="ptmb7t-x-x-120">Conceptual Framework, Design and Methods. </span>The specific machinery underpinning our approach relies
on variational inference. The main justification for variational inference is that there is a clear objective
function that is being optimised and that this objective function is in fact a lower bound on the true marginal
likelihood. The use of variational inference has proved successful in a variety of applications
<span class="cite">[<a 
href="#Xnickisch2008approximations">7</a>,&#x00A0;<a 
href="#Xnguyen2013efficient">5</a>,&#x00A0;<a 
href="#Xlazaro2012bayesian">3</a>,&#x00A0;<a 
href="#Xgirolami2006variational">1</a>]</span>.
<!--l. 257--><p class="indent" >   We will target joint inversion problems and supervised learning scenarios where we are given a dataset
<span 
class="zpzccmry-x-x-120"><img 
src="zpzccmry-c-44.png" alt="D" class="-120x-x-44" /> </span><span 
class="zptmcmr-x-x-120">= </span><span 
class="zpzccmry-x-x-120">{</span><img 
src="proposal0x.png" alt="x"  class="vec" ><sub><span 
class="zptmcmrm-x-x-90">i</span></sub><span 
class="zptmcmrm-x-x-120">,</span><img 
src="proposal1x.png" alt="y"  class="vec" ><sub><span 
class="zptmcmrm-x-x-90">i</span></sub><span 
class="zpzccmry-x-x-120">}</span><sub><span 
class="zptmcmrm-x-x-90">i</span><span 
class="zptmcmr-x-x-90">=1</span></sub><sup><span 
class="zptmcmrm-x-x-90">n</span></sup> and knowledge of the forward models <img 
src="proposal2x.png" alt="g"  class="vec" ><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal3x.png" alt="f"  class="vec" ><span 
class="zptmcmr-x-x-120">) </span>that will impose constraints on our observations
<img 
src="proposal4x.png" alt="y"  class="vec" >. This can be understood with the data flow:
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
<img 
src="proposal5x.png" alt="x &#x2192; f &#x2192; g &#x2192;  y,
" class="math-display" ></center></td></tr></table>
<!--l. 263--><p class="nopar" >
where <img 
src="proposal6x.png" alt="x"  class="vec" > are the locations of our observations; <img 
src="proposal7x.png" alt="f"  class="vec" > are the latent functions (or parameters we are interested in); <img 
src="proposal8x.png" alt="g"  class="vec" >
are the forward models; and <img 
src="proposal9x.png" alt="y"  class="vec" > are our observations. Note that all these quantities are multi-dimensional
variables, meaning that we are dealing with multi-dimensional inputs, multi-dimensional parameters; and
multi-dimensional outputs.
<!--l. 268--><p class="indent" >   An illustration of the conceptual framework of our approach is given in Figure <a 
href="#x1-20011">1<!--tex4ht:ref: fig:inference --></a>(b), where we will make
use of the following assumptions and methodology:
<dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">GP priors. </span>The latent functions (or parameters) <img 
src="proposal10x.png" alt="f"  class="vec" ><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal11x.png" alt="x"  class="vec" ><span 
class="zptmcmr-x-x-120">) </span>are drawn from Gaussian process (GP) priors.
This  will  allow  us  for  a  great  flexibility  in  the  types  of  problems  we  can  model,  as  GPs  are
(non-parametric) priors over (non-linear) functions.
                                                                                            
                                                                                            
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Factorisation over latent functions. </span>A priori, these functions are statistically independent, i.e.&#x00A0;our
prior factorises over latent functions but not over data-points.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Evaluation of forward models. </span>We do not need to know the explicit form of the forward models <img 
src="proposal12x.png" alt="g"  class="vec" ><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal13x.png" alt="f"  class="vec" ><span 
class="zptmcmr-x-x-120">)</span>,
requiring only their evaluation as well as their corresponding likelihood <span 
class="zptmcmrm-x-x-120">p</span><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal14x.png" alt="y"  class="vec" ><span 
class="zpzccmry-x-x-120">|</span><img 
src="proposal15x.png" alt="g"  class="vec" ><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal16x.png" alt="f"  class="vec" ><span 
class="zptmcmr-x-x-120">))</span>.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Factorisation  of  the  likelihood.  </span>Observations  <img 
src="proposal17x.png" alt="y"  class="vec" >  are  conditionally  independent  given  knowledge  of
the latent functions <img 
src="proposal18x.png" alt="g"  class="vec" ><span 
class="zptmcmr-x-x-120">(</span><img 
src="proposal19x.png" alt="f"  class="vec" ><span 
class="zptmcmr-x-x-120">)</span>. In other words, our likelihood factorises over observations but not over the
output dimensions.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Mixture-of-Gaussians posterior. </span>Our approximate posterior will belong to either the Gaussian family
or the Mixture-of-Gaussians family.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Variational Inference. </span>We will learn our posterior through variational inference, which effectively
accounts for optimisation of a lower bound on the true marginal likelihood of the model.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Scalability. </span>We will scale up our inference methods to deal with very large datasets through the use
of summary statistics (inducing points), which yields a decomposition of the variational objective
function and allows the use of stochastic optimisation methods.</dd></dl>
<!--l. 302--><p class="noindent" >With these assumptions it is natural to ask: What types of problems can we address? The answer is a <span 
class="ptmri7t-x-x-120">a large</span>
<span 
class="ptmri7t-x-x-120">variety of tasks</span>. From non-linear joint inversions to multi-output regression to multi-class classification to
count data and beyond. In fact, this learning machinery will be able to carry out posterior inference even for
problems that are they to be defined. This is because we will not need to know the specifics of the forward
models (or likelihoods) and our only requirement is to be able to evaluate them at a latent function value.
<br 
class="newline" /><br 
class="newline" /><span 
class="ptmb7t-x-x-120">Evaluation. </span>We will evaluate our approach using sound methodologies such as cross-validation and publicly
available data on inversion problems, multi-output regression, multi-class classification, and modelling of
count data. We will compare to state-of-the-art stochastic approaches such as MCMC in terms of accuracy of
the predictions as well as computational cost. We will also compare to hard-coded (problem-specific)
deterministic posterior inference approaches.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-5000"></a><span 
class="ptmrc7t-x-x-120">E5. E<span 
class="small-caps">x</span><span 
class="small-caps">p</span><span 
class="small-caps">e</span><span 
class="small-caps">c</span><span 
class="small-caps">t</span><span 
class="small-caps">e</span><span 
class="small-caps">d</span> O<span 
class="small-caps">u</span><span 
class="small-caps">t</span><span 
class="small-caps">c</span><span 
class="small-caps">o</span><span 
class="small-caps">m</span><span 
class="small-caps">e</span><span 
class="small-caps">s</span></span></h5>
<!--l. 321--><p class="noindent" >Corresponding to the aims of the this project, the expected outcomes are as follows:
<dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Basic research output. </span>The development, analysis, and evaluation of automatic inference techniques
for a wide variety of problems such as probabilistic nonlinear joint inversions, multi-output regression,
multi-class classification, and modelling of count data.
</dd><dt class="enumerate-enumitem">
</dt><dd 
class="enumerate-enumitem"><span 
class="ptmri7t-x-x-120">Scalable code. </span>The delivery of highly parallelisable software that can scale to very large datasets,
making it useful for large-scale data-fusion problems.</dd></dl>
                                                                                            
                                                                                            
<!--l. 331--><p class="noindent" ><span 
class="ptmb7t-x-x-120">National benefit. </span>Joint inversion problems of national interest are common in the exploration of natural and
mineral resources, where one wishes to infer geophysical parameters in the Earth&#8217;s subsurface by measuring
related physical quantities well before a drilling campaign starts. The importance of probabilistic outputs can
also be encountered in purely predictive tasks, such as modelling of solar irradiance in the Australian
continent.
<!--l. 338--><p class="indent" >   In both cases, the joint inversion problem and the purely predictive problem, there is a need for having
inference algorithms capable of dealing with very complex forward models (or likelihood functions) without
resorting to expensive MCMC simulations. More importantly, as the number of measurements available in
these types of applications gets bigger and bigger, scalability becomes a necessary component of modern
inference algorithms.
<!--l. 344--><p class="indent" >   It is necessary to emphasise that this project is the first stage of a much more ambitious goal, that of
introducing the so-called &#8216;big-data&#8217; technologies into scientific research. By having scalable probabilistic
inference algorithms that (a) are applicable to a wide range of problems; and (b) can be easy to parallelise, we
aim to change the way many areas of science currently carry out the quantification and propagation of
uncertainty. Our ultimate goal is to reduce the computational time and/or human time involved in the
process of evaluating uncertainty through MCMC simulations and/or lengthy model-specific
derivations.
   <h5 class="likesubsubsectionHead"><a 
 id="x1-6000"></a><span 
class="ptmrc7t-x-x-120">E6. D<span 
class="small-caps">e</span><span 
class="small-caps">s</span><span 
class="small-caps">c</span><span 
class="small-caps">r</span><span 
class="small-caps">i</span><span 
class="small-caps">p</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">o</span><span 
class="small-caps">n</span> <span 
class="small-caps">o</span><span 
class="small-caps">f</span> P<span 
class="small-caps">e</span><span 
class="small-caps">r</span><span 
class="small-caps">s</span><span 
class="small-caps">o</span><span 
class="small-caps">n</span><span 
class="small-caps">n</span><span 
class="small-caps">e</span><span 
class="small-caps">l</span></span></h5>
<!--l. 357--><p class="noindent" ><span 
class="ptmri7t-x-x-120">Chief Investigator</span>. The CI of this project, Dr.&#x00A0;Edwin Bonilla, will take full responsibility for the conduct of
the research. In particular, he will investigate the techniques proposed, carry out the corresponding analysis
and develop the analytical tools required to achieve the aims of this project. He will closely supervise the
Research Assistant in order to deliver the expected outcomes outlined in section E5. Dr.&#x00A0;Bonilla&#8217;s extensive
expertise in modelling with Gaussian process (GP) priors is crucial to this project and his recent work on
scaling up GP-based regression models <span class="cite">[<a 
href="#Xnguyen-bonilla-uai-2014">6</a>]</span> will underpin the <span 
class="ptmri7t-x-x-120">efficiency </span>and <span 
class="ptmri7t-x-x-120">scalability </span>component of the
project. <br 
class="newline" /><br 
class="newline" /><span 
class="ptmri7t-x-x-120">Research Assistant</span>. The 50% Research Assistant (RA) will be responsible for collaborating closely with the
CI and for the development of the corresponding algorithms. It is expected that this RA will carry out
research activities, discussing existing approaches and alternative methods for the proposed research.
Therefore, as can be seen in Section B1, a level A step 1 academic has been requested for this
position.
<!--l. 1--><p class="noindent" >
   <h5 class="likesubsubsectionHead"><a 
 id="x1-7000"></a><span 
class="ptmrc7t-x-x-109">E7. R<span 
class="small-caps">e</span><span 
class="small-caps">f</span><span 
class="small-caps">e</span><span 
class="small-caps">r</span><span 
class="small-caps">e</span><span 
class="small-caps">n</span><span 
class="small-caps">c</span><span 
class="small-caps">e</span><span 
class="small-caps">s</span></span></h5>
<!--l. 1--><p class="noindent" >
     <div class="thebibliography">
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[1]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xgirolami2006variational"></a><span 
class="ptmr7t-x-x-109">Mark Girolami and Simon Rogers. Variational Bayesian multinomial probit regression with Gaussian</span>
     <span 
class="ptmr7t-x-x-109">process priors. </span><span 
class="ptmri7t-x-x-109">Neural Computation</span><span 
class="ptmr7t-x-x-109">, 18(8):1790&#8211;1817, 2006.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[2]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xjordan1998introduction"></a><span 
class="ptmr7t-x-x-109">Michael</span><span 
class="ptmr7t-x-x-109">&#x00A0;I Jordan, Zoubin Ghahramani, Tommi</span><span 
class="ptmr7t-x-x-109">&#x00A0;S Jaakkola, and Lawrence</span><span 
class="ptmr7t-x-x-109">&#x00A0;K Saul.  </span><span 
class="ptmri7t-x-x-109">An introduction</span>
     <span 
class="ptmri7t-x-x-109">to variational methods for graphical models</span><span 
class="ptmr7t-x-x-109">. Springer, 1998.</span>
     </p>
                                                                                            
                                                                                            
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[3]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xlazaro2012bayesian"></a><span 
class="ptmr7t-x-x-109">Miguel L</span><span 
class="ptmr7t-x-x-109">ázaro-Gredilla.  Bayesian warped Gaussian processes.  In </span><span 
class="ptmri7t-x-x-109">Advances in Neural Information</span>
     <span 
class="ptmri7t-x-x-109">Processing Systems</span><span 
class="ptmr7t-x-x-109">, 2012.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[4]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xneal1993probabilistic"></a><span 
class="ptmr7t-x-x-109">Radford</span><span 
class="ptmr7t-x-x-109">&#x00A0;M.  Neal.   Probabilistic  inference  using  Markov  chain  Monte  Carlo  methods.   Technical</span>
     <span 
class="ptmr7t-x-x-109">report, Department of Computer Science, University of Toronto, 1993.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[5]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xnguyen2013efficient"></a><span 
class="ptmr7t-x-x-109">Trung</span><span 
class="ptmr7t-x-x-109">&#x00A0;V. Nguyen and Edwin Bonilla.  Efficient variational inference for Gaussian process regression</span>
     <span 
class="ptmr7t-x-x-109">networks. In </span><span 
class="ptmri7t-x-x-109">International Conference on Artificial Intelligence and Statistics</span><span 
class="ptmr7t-x-x-109">, 2013.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[6]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xnguyen-bonilla-uai-2014"></a><span 
class="ptmr7t-x-x-109">Trung</span><span 
class="ptmr7t-x-x-109">&#x00A0;V.  Nguyen  and  Edwin</span><span 
class="ptmr7t-x-x-109">&#x00A0;V.  Bonilla.     Collaborative  multi-output  Gaussian  processes.     In</span>
     <span 
class="ptmri7t-x-x-109">Uncertainty in Artificial Intelligence</span><span 
class="ptmr7t-x-x-109">, 2014.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[7]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xnickisch2008approximations"></a><span 
class="ptmr7t-x-x-109">Hannes  Nickisch  and  Carl</span><span 
class="ptmr7t-x-x-109">&#x00A0;Edward  Rasmussen.     Approximations  for  binary  Gaussian  process</span>
     <span 
class="ptmr7t-x-x-109">classification. </span><span 
class="ptmri7t-x-x-109">Journal of Machine Learning Research</span><span 
class="ptmr7t-x-x-109">, 9(10):2035&#8211;2078, 2008.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[8]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xopper-arch-nc-2009"></a><span 
class="ptmr7t-x-x-109">Manfred Opper and C</span><span 
class="ptmr7t-x-x-109">édric Archambeau.  The variational Gaussian approximation revisited.  </span><span 
class="ptmri7t-x-x-109">Neural</span>
     <span 
class="ptmri7t-x-x-109">Computation</span><span 
class="ptmr7t-x-x-109">, 21(3):786&#8211;792, 2009.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
  <span 
class="ptmr7t-x-x-109">[9]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="Xranganath2014black"></a><span 
class="ptmr7t-x-x-109">Rajesh Ranganath, Sean Gerrish, and David</span><span 
class="ptmr7t-x-x-109">&#x00A0;M. Blei. Black box variational inference. In </span><span 
class="ptmri7t-x-x-109">International</span>
     <span 
class="ptmri7t-x-x-109">Conference on Artificial Intelligence and Statistics</span><span 
class="ptmr7t-x-x-109">, 2014.</span>
     </p>
     <p class="bibitem" ><span class="biblabel">
 <span 
class="ptmr7t-x-x-109">[10]</span><span class="bibsp"><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span><span 
class="ptmr7t-x-x-109">&#x00A0;</span></span></span><a 
 id="XSchRouBac2013"></a><span 
class="ptmr7t-x-x-109">Mark</span><span 
class="ptmr7t-x-x-109">&#x00A0;W. Schmidt, Nicolas</span><span 
class="ptmr7t-x-x-109">&#x00A0;Le Roux, and Francis Bach.  Minimizing finite sums with the stochastic</span>
     <span 
class="ptmr7t-x-x-109">average gradient. </span><span 
class="ptmri7t-x-x-109">Computing Research Repository</span><span 
class="ptmr7t-x-x-109">, 2013.</span>
</p>
     </div>
    
</body></html> 

                                                                                            


